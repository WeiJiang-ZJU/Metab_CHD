{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "import pandas as pd\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "import matplotlib\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "import numpy as np\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "import scipy as sp\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "import sklearn\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "GROUP = {0:[0], 1:[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymetrics(y_true, y_pred):\n",
    "    TP = sum([1 if y_pred[i]==1 and y_true[i]==1 else 0 for i in range(len(y_pred))]) #True Positive\n",
    "    FP = sum([1 if y_pred[i]==1 and y_true[i]==0 else 0 for i in range(len(y_pred))]) #False Positive\n",
    "    TN = sum([1 if y_pred[i]==0 and y_true[i]==0 else 0 for i in range(len(y_pred))]) #True Negative\n",
    "    FN = sum([1 if y_pred[i]==0 and y_true[i]==1 else 0 for i in range(len(y_pred))]) #False Negative\n",
    "    TPR = TP / float(TP + FN) if TP + FN != 0 else 0.0 #True Positive Rate\n",
    "    TNR = TN / float(TN + FP) if TN + FP != 0 else 0.0 #True Negative Rate\n",
    "    FPR = FP / float(FP + TN) if FP + TN != 0 else 0.0 #False Positive Rate\n",
    "    FNR = FN / float(FN + TP) if FN + TP != 0 else 0.0 #False Negative Rate\n",
    "    Percision = TP / float(TP + FP) if TP + FP != 0 else 0.0\n",
    "    Recall = TP / float(TP + FN) if TP + FN != 0 else 0.0\n",
    "    return f'TPR = {round(TPR, 3)}, TNR = {round(TNR, 3)}, Percision = {round(Percision, 3)}, Recall = {round(Recall, 3)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(X, y, group={0:[0], 1:[2]}):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        if y.iloc[i] in group[0]:\n",
    "            new_X.append(X.iloc[i, :])\n",
    "            new_y.append(0)\n",
    "        if y.iloc[i] in group[1]:\n",
    "            new_X.append(X.iloc[i, :])\n",
    "            new_y.append(1)\n",
    "    return np.array(new_X), np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('path/to/file', index_col=0)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "labels = data['dia_result']\n",
    "variances = data.drop(columns=['dia_result']).var()\n",
    "threshold = 0.01\n",
    "high_variance_features = variances[variances > threshold].index\n",
    "filtered_data = data.drop(columns='dia_result')\n",
    "print(filtered_data)\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(filtered_data)\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=filtered_data.columns)\n",
    "standardized_df['dia_result'] = labels.reset_index(drop=True)\n",
    "print(standardized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'dia_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Non-VSD: {(standardized_df[target]==0).sum()}; VSD: {(standardized_df[target]==2).sum()}'\n",
    "\n",
    "cv = model_selection.StratifiedShuffleSplit(n_splits=2, test_size=0.4, random_state=SEED) \n",
    "X = standardized_df.drop(columns=[target])\n",
    "y = standardized_df[target] \n",
    "\n",
    "for train_index, test_index in cv.split(X, y):pass \n",
    "X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "f'Train: {X_train.shape}, Distributionï¼š({[(y_train==i).sum() for i in [0, 2]]}); Test: {X_test.shape}, Distribution:({[(y_test==i).sum() for i in [0, 2]]})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, y_train_ = get_x_y(X_train, y_train, GROUP)\n",
    "X_test_, y_test_ = get_x_y(X_test, y_test, GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(f\"Training set class distribution: {Counter(y_train_)}\")\n",
    "print(f\"Test set class distribution: {Counter(y_test_)}\")\n",
    "\n",
    "smote = SMOTE(random_state=SEED)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_, y_train_)\n",
    "\n",
    "print(pd.Series(y_train_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    svm.SVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "    ]\n",
    "\n",
    "cv_split = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.4, train_size =0.6, random_state=SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train AUC Mean', 'MLA Valid AUC Mean', \n",
    "               'MLA Valid AUC 3*STD', 'MLA Time', 'MLA Test AUC']\n",
    "MLA_compare = pd.DataFrame(columns=MLA_columns)\n",
    "\n",
    "row_index = 0\n",
    "\n",
    "for alg in MLA:\n",
    "    try:\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "        \n",
    "        cv_results = model_selection.cross_validate(\n",
    "            alg, X_train_smote, y_train_smote, cv=cv_split, scoring='roc_auc', return_train_score=True\n",
    "        )\n",
    "        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "        MLA_compare.loc[row_index, 'MLA Train AUC Mean'] = cv_results['train_score'].mean()\n",
    "        MLA_compare.loc[row_index, 'MLA Valid AUC Mean'] = cv_results['test_score'].mean()\n",
    "        MLA_compare.loc[row_index, 'MLA Valid AUC 3*STD'] = cv_results['test_score'].std() * 3\n",
    "        \n",
    "        alg.fit(X_train_smote, y_train_smote)\n",
    "        y_test_pred = alg.predict_proba(X_test_)[:, 1]\n",
    "        test_auc = roc_auc_score(y_test_, y_test_pred)\n",
    "        MLA_compare.loc[row_index, 'MLA Test AUC'] = test_auc\n",
    "    except Exception as e:\n",
    "        print(f\"Model {alg.__class__.__name__} Error: {e}\")\n",
    "        MLA_compare.loc[row_index, 'MLA Test AUC'] = None\n",
    "    \n",
    "    finally:\n",
    "        row_index += 1\n",
    "        \n",
    "MLA_compare.sort_values(by=['MLA Test AUC'], ascending=False, inplace=True)\n",
    "print(MLA_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA_compare_test = MLA_compare.dropna(subset=[\"MLA Test AUC\"])\n",
    "fig = plt.figure(figsize=(10, 6), dpi=300)\n",
    "_ = sns.barplot(x='MLA Test AUC', y = 'MLA Name', data = MLA_compare_test, color = 'm') \n",
    "_ = plt.title('Machine Learning Algorithm Accuracy Score in Test Set \\n')\n",
    "_ = plt.xlabel('AUC')\n",
    "_ = plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "model = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "param_space = {\n",
    "    'C': (1e-3, 1e3, 'log-uniform'),\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': [None]\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    scoring='roc_auc',\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (ROC AUC):\", bayes_search.best_score_)\n",
    "\n",
    "best_model = bayes_search.best_estimator_\n",
    "y_test_pred = best_model.predict_proba(X_test_)[:, 1]\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test_, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_test_pred = best_model.predict_proba(X_test_)[:, 1]\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test_, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_, y_test_pred, pos_label=1)\n",
    "fig = plt.figure(figsize=(8, 6), dpi=300)\n",
    "_ = plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % metrics.auc(fpr, tpr)) \n",
    "_ = plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\") \n",
    "_ = plt.xlim([-0.01, 1.01])\n",
    "_ = plt.ylim([-0.01, 1.01]) \n",
    "_ = plt.xlabel(\"False Positive Rate\")\n",
    "_ = plt.ylabel(\"True Positive Rate\") \n",
    "_ = plt.legend(loc=\"lower right\") \n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_, y_test_pred, pos_label=1)\n",
    "youden_index = tpr - fpr\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "y_pred_optimal = (y_test_pred >= optimal_threshold).astype(int)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_, y_pred_optimal))\n",
    "f'Test Accuracy = {best_model.score(X_test_, y_test_)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External DataSet Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predict = pd.read_csv('path/to/file', index_col=0)\n",
    "X_predict = data_predict[X_train.columns]\n",
    "y_predict = data_predict[target]\n",
    "scaler = StandardScaler()\n",
    "standardized_data_pd = scaler.fit_transform(X_predict)\n",
    "standardized_df_pd = pd.DataFrame(standardized_data_pd, columns=X_predict.columns)\n",
    "X_predict_, y_predict_ = get_x_y(standardized_df_pd, y_predict, GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = best_model.predict(X_predict_)\n",
    "probs = best_model.predict_proba(X_predict_)[:, 1]\n",
    "metrics_result = mymetrics(y_predict_, predictions)\n",
    "print(f'External Data Metrics: {metrics_result}')\n",
    "y_pred_optimal = (probs >= optimal_threshold).astype(int)\n",
    "print(\"Classification Report:\\n\", classification_report(y_predict_, y_pred_optimal))\n",
    "print(\"External Test ROC AUC:\", roc_auc_score(y_predict_, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_predict_, probs, pos_label=1)\n",
    "fig = plt.figure(figsize=(8, 6), dpi=300)\n",
    "_ = plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % metrics.auc(fpr, tpr))\n",
    "_ = plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "_ = plt.xlim([-0.01, 1.01])\n",
    "_ = plt.ylim([-0.01, 1.01])\n",
    "_ = plt.xlabel(\"False Positive Rate\")\n",
    "_ = plt.ylabel(\"True Positive Rate\")\n",
    "_ = plt.legend(loc=\"lower right\")\n",
    "_ = plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
